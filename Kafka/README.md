# Kafka Tutorial


## Introduction

Kafka is a distributed streaming platform to send data from some message producers to some message consumers.  
It was created by LinkedIn and is now open source and used by many major companies (AirBnB, Netflix, Twitter, ...).

Kafka is distributed across multiple brokers (servers in a cluster).  
It is fault-tolerant by replicating data across brokers.  
It is horizontally scalable by adding or removing brokers to the cluster.  
It is quasi real-time with high performance at the ms level.  
It can scale to millions of messages per second.

Kafka can be used as a messaging system, for activity tracking, metric gathering, application logs, stream processing, service decoupling ...  
It also integrates with Big Data technologies like Hadoop.


## Kafka Architecture


### Topics

A topic is a stream of data identified by a unique name.  
A cluster can have any number of topics.  
A message is always sent to a given topic, and it supports any type of data (text, JSON, Protobuf, binary).  
A sequence of messages in a topic is called a "data stream".  

Once a topic is being used by some consumers, it must never change the type of its key or value, or it would break consumers.  
In that case, a new topic should be created instead, and consumers should be adjusted to use this new topic.

Each topic has a replication factor, it is the number of different brokers storing each partition of this topic.    
The replication factor must be higher than 1 to ensure fault tolerance in case of a broker failure.   

Only one broker can be the leader for a given partition of a topic.  
Producers only send messages to the leader broker for a given partition.  
Other brokers storing this partition are only replicas in case the leader goes down.  
After each message insertion in a leader broker, Kafka will replicate this message to every replica.  
By default, consumers request messages only from the leader broker for each partition.

Since Kafka 2.4, consumers can read from the closest replica (instead of the leader) to improve latency.


### Partitions

A partition is a division of a topic, a topic can have any number of partitions.  
The partition to which a message belongs is determined by a hash of its message key.  
2 messages sent to a topic with the same key will always be part of the same partition.  
Kafka messages are guaranteed to be ordered within a partition, but not across partitions !  
A message only belongs to a single partition of a topic.  

Once a message is written to a partition, it can never be modified or deleted.  
Messages in Kafka are kept only for a limited time (by default one week), then they get purged.

### Message Offsets

The offset of a message is its position within its partition.  
Offsets are not re-used even after older messages get purged, they are always increasing within a partition.


### Kafka Producers

A producer is a process writing some messages to a topic.  
The producer decides what partition the message belongs to by hashing its message key.  
If no message key is provided, the producer will choose a partition, for example with a round-robin strategy.  

A producer can choose to receive an ack of each message sent to kafka :
- `acks=0` : the producer does not wait for any ack (possible data loss)
- `acks=1` : the producer waits for the ack from the leader broker (rare data loss)
- `acks=all` : the producer waits for the ack from the leader and all replicas (no data loss)


### Kafka Messages

A message is a single piece of data sent to a topic by a producer.  

The message generated by the producer is made of :
- a message key (to decide the partition)
- a message value
- a compression type (none, gzip, ...)
- K/V headers (optional)
- a partition and an offset
- a timestamp

Messages must be serialized (using Kafka serializers) before sending them to a producer.  
Messages must be deserialized after retrieving them with a consumer.

A message key should be set to ensure partial ordering between the messages.  
The key can be of any type (text, number, binary...).  
For example, if events are sent to identify the position of a float of drones every second, we can use the drone ID as a key,
so the successive positions of a given drone are ordered.


### Kafka Consumers

A consumer is a process reading the messages from a topic.  
Consumers request messages to the Kafka brokers (brokers do not push messages to consumers).  
Messages are read in order from lowest to highest offsets within each partition.  
Consumers deserialize the binary data they receive from the brokers.  
If the key is an integer and the message value is a string, we would use an IntegerDeserializer and a StringDeserializer. 

### Kafka Consumer Groups

A consumer group is a group of consumers reading from exclusive partitions of a given topic.  
This provides horizontal scalability to the consumer tasks, allowing the processing of messages in a topic to be performed in parallel by multiple consumers.  
If there are more consumers in a group than partitions in the topic, some consumers of the group will be idle.  

Every message in the topic is read by a single consumer of the consumer group.  
Several consumer groups can read from the same topic (they are just independent groups).  
A consumer group is identified by a group ID.


### Consumer Offsets

The consumer offsets are the offsets up to which a consumer has been reading messages in each partition of a topic.  
They are stored in an internal topic called `__consumer_offsets`.  
When the consumer restarts, it will start processing messages for each partition from these consumer offsets.  
Each consumer regularly updates its consumer offset for each partition it processes.

There are 3 strategies to update this consumer offset :
- at least once (default) : guarantees that all messages are processed at least once, the consumer offset is updated after successful processing of each message.
- at most once :  guarantees that all messages are processed not more than once, the consumer offset is updated as soon as a message iss received.
- exactly once : use transactional API, only done for Kaftka to Kafka worflow.

The most common way to handle it is to use the "at least once" strategy and have an idempotent behavior in case a message is processed twice.


### Kafka Brokers and Cluster

A cluster is a group of Kafka brokers (servers).  
Each broker is identified by an ID and contains some partitions of the topics.  
A cluster can have any number of brokers (over 100 for big clusters).  

In recent Kafka versions, each broker can be used as a "bootstrap broker".  
It knows about the entire cluster and is an entry point to any Kafka query on the cluster.


### Zookeeper and Kafka Raft

Zookeeper is a dedicated software for the management of the Kafka cluster :  
- it helps in the leader election for each partition
- it sends messages across the cluster on changes (new or deleted topic, broker dies or starts...)

It used to be a mandatory element to run a Kafka cluster before Kafka 3.  
Kafka plans to replace Zoopeeper by Kafka Raft (or Kraft).  
Since Kafka 3, Kraft can replace Zookeeper, and is production-ready since Kafka 3.3.1 (2022).  
From Kafka 4, there will be no more support for Zookeeper (not released yet as of 2023).

Zookeeper shows some scaling issues for big cluster with more than 100,000 partitions.  
Without Zookeeper, Kafka can handle millions of partitions.  
Getting rid of Zookeeper also makes the Kafka configuration much easier to monitor and support.  
With Kraft, there is no longer a software managing the cluster.  
Instead, each broker is able to act as an entry point for any operation on the cluster. 


## Kafka Setup

Kafka is open-source, so it is free to install on Windows, MacOS and Linux.  
The production configuration of Kafka is tedious and can take several hours.  
For development and testing, we can simply create a cluster locally with a single broker.


### Using Conduktor

[Conduktor](https://www.conduktor.io/) is a web service offering a Kafka GUI to start and manage Kafka clusters in the cloud.    
It follows a monthly payment system but has a free tiers with a single cluster.  

We can create a free account and create a playground to start a Kafka cluster.


### Locally on MacOS with a single broker

#### Option 1 : Manual Installation

Kafka requires Java JDK, which can be installed for example from [AWS Corretto](https://aws.amazon.com/corretto).  
Download the pkg file for the latest JDK and install it, then check that it worked with `java --version` in the console.

Kafka binaries are available from the [Kafka download page](https://kafka.apache.org/downloads).  
Download the tgz archive, extract it, copy it to the desired location and add the `bin/` folder to the `PATH` env variable.

Start Zookeeper in a console with :
```commandline
zookeeper-server-start.sh <PATH_TO_KAFKA>/config/zookeeper.properties
```

Start a Kafka broker in another console with :
```commandline
kafka-server-start.sh <PATH_TO_KAFKA>/config/server.properties
```

The `dataDir` and `log.dir` properties can be modified in the Zookeeper and server properties files.


#### Option 2 : Installation with Homebrew

Alternatively, we can install the Kafka binaries with Homebrew :
```commandline
brew install kafka
```
Homebrew installs its packages under `/usr/local/opt/` and its config files under `/usr/local/etc/`.    
It automatically adds its `bin` folders to the `PATH` variable.    
Binaries installed with Homebrew are called without the `.sh` extension.

To start Zookeeper installed with Homebrew ;
```commandline
zookeeper-server-start /usr/local/etc/zookeeper/zoo.cfg
```

Start a Kafka broker in another console with :
```commandline
kafka-server-start /usr/local/etc/kafka/server.properties
```


## Kafka CLI

Since Zookeeper is getting deprecated, client commands should use the `--bootstrap-server` option instead of `--zookeeper` to use Kraft.

When using a secure cluster (for example a cluster managed on Conduktor), we should create a config file with the broker properties.  
These properties are available in Conduktor under `Kafka Cluster > Advanced properties` and look like :

###### conduktor-cluster.config
```commandline
security.protocol=SASL_SSL
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='xxx' password='xxx.xxx.xxx';
```

All below CLI commands are using a local Kafka cluster so they only require `--bootstrap-server localhost:9092`.  
To run the query for a Conduktor cluster, replace it by `--bootstrap-server cluster.playground.cdkt.io:9092 --command-config /tmp/conduktor-cluster.config`.  
For example, to describe all existing topics :

```commandline
kafka-topics --describe --bootstrap-server cluster.playground.cdkt.io:9092 --command-config /tmp/conduktor-cluster.config
```

### Topic management

Topics are managed with the `kafka-topics` Kafka CLI command. 

#### Create a topic

The topic creation uses the `--create` command with the `--topic <TOPIC_NAME>` parameter.  
The number of partitions of the topic can be specified with the `--partitions <INT>` parameter.  
The replication factor can be set with `--replication-factor <INT>` and cannot be more than the number of brokers.

```commandline
kafka-topics --bootstrap-server localhost:9092 --create --topic test-topic
``` 

#### List existing topics

The `--list` command simply list the name of existing topics.  
To get more detailed info about each topic (leader, partitions, replicas...), use the `--describe` command instead.

```commandline
kafka-topics --bootstrap-server localhost:9092 --list
kafka-topics --bootstrap-server localhost:9092 --describe 
``` 

#### Delete a topic

The topic deletion uses the `--delete` command with the `--topic <TOPIC_NAME>` parameter.  

```commandline
kafka-topics --bootstrap-server localhost:9092 --delete --topic test-topic
```

### Kafka Producer

A console producer can be used with the `kafka-console-producer` Kafka CLI command.

To write some messages to a topic, we can use the below command.  
It opens a stream, and we can write a message per line, then Ctrl-C to stop sending messages.

```commandline
kafka-console-producer  --bootstrap-server localhost:9092 --topic test-topic
> my message 1
> my message 2
> Ctrl-C
```

We can force the producer to wait for all brokers to ack with `--producer-property acks=all`.

We can specify a key for each message by adding the `--property parse.key=true` and `--property key.separator=:` arguments :

```commandline
kafka-console-producer  --bootstrap-server localhost:9092 --topic test-topic --property parse.key=true --property key.separator=:
> keyA:my message 1
> keyA:my message 2
> keyB:my message 1
> Ctrl-C
```

### Kafka consumer

A console consumer can be used with the `kafka-console-consumer` Kafka CLI command.  
It is a daemon process that starts waiting for incoming messages in the topic.  
By default, it starts consuming from the time it starts, but we can consume from the beginning with the `--from-beginning` parameter.

We can print some metadata in addition to each message value by using a formatter.

```commandline
// consume messages from now
kafka-console-consumer  --bootstrap-server localhost:9092 --topic test-topic

// consume messages from the beginning
kafka-console-consumer  --bootstrap-server localhost:9092 --topic test-topic --from-beginning

// consume messages and display metadata
kafka-console-consumer  --bootstrap-server localhost:9092 --topic test-topic --from-beginning
                        --formatter kafka.tools.DefaultMessageFormatter --property timestamp.print=true
                        --property key.print=true --property print.partition=true
```
 
If the topic has multiple partitions, messages within a partition are consumed in order, but no order is guaranteed for messages between partitions.

We can use the `--group <GROUP_NAME>` parameter to include the consumer in a consumer group.  
All consumers started with the same group name will be part of the same consumer group.  
They will all be in charge of a distinct set of partitions of the consumed topic.  

Consumer groups can be managed with the `kafka-consumer-groups` CLI command.  
It supports the `--list` and the `--describe` commands.

```commandline
// list all consumer groups
kafka-consumer-groups --bootstrap-server localhost:9092 --list

// describe a consumer group (topic, partition, offset...)
kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group my-group
```

